{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nPyTorch: \u5b9a\u4e49\u65b0\u7684autograd\u51fd\u6570\n----------------------------------------\n\n\u672c\u4f8b\u4e2d\u7684\u5168\u8fde\u63a5\u795e\u7ecf\u7f51\u7edc\u6709\u4e00\u4e2a\u9690\u85cf\u5c42, \u540e\u63a5ReLU\u6fc0\u6d3b\u5c42, \u5e76\u4e14\u4e0d\u5e26\u504f\u7f6e\u53c2\u6570. \n\u8bad\u7ec3\u65f6\u901a\u8fc7\u6700\u5c0f\u5316\u6b27\u5f0f\u8ddd\u79bb\u7684\u5e73\u65b9, \u6765\u5b66\u4e60\u4ecex\u5230y\u7684\u6620\u5c04.\n\n\u5728\u6b64\u5b9e\u73b0\u4e2d, \u6211\u4eec\u4f7f\u7528PyTorch\u53d8\u91cf\u4e0a\u7684\u51fd\u6570\u6765\u8fdb\u884c\u524d\u5411\u8ba1\u7b97, \u7136\u540e\u7528PyTorch\u7684autograd\u8ba1\u7b97\u68af\u5ea6\n\n\u6211\u4eec\u8fd8\u5b9e\u73b0\u4e86\u4e00\u4e2a\u5b9a\u5236\u5316\u7684autograd\u51fd\u6570, \u7528\u4e8eReLU\u51fd\u6570.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nfrom torch.autograd import Variable\n\n\nclass MyReLU(torch.autograd.Function):\n    \"\"\"\n    \u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u5b50\u7c7b\u5b9e\u73b0\u6211\u4eec\u81ea\u5df1\u5b9a\u5236\u7684autograd\u51fd\u6570\n    torch.autograd.Function\u548c\u6267\u884c\u5728Tensors\u4e0a\u8fd0\u884c\u7684\u5411\u524d\u548c\u5411\u540e\u901a\u884c\u8bc1.\n    \"\"\"\n\n    @staticmethod\n    def forward(ctx, input):\n        \"\"\"\n        \u5728\u6b63\u5411\u4f20\u9012\u4e2d,\u6211\u4eec\u6536\u5230\u4e00\u4e2a\u5305\u542b\u8f93\u5165\u548c\u8fd4\u56de\u5f20\u91cf\u7684\u5f20\u91cf,\u5176\u4e2d\u5305\u542b\u8f93\u51fa.\n        ctx\u662f\u4e00\u4e2a\u4e0a\u4e0b\u6587\u5bf9\u8c61,\u53ef\u7528\u4e8e\u5b58\u50a8\u53cd\u5411\u8ba1\u7b97\u7684\u4fe1\u606f.\n        \u60a8\u53ef\u4ee5\u4f7f\u7528ctx.save_for_backward\u65b9\u6cd5\u7f13\u5b58\u4efb\u610f\u5bf9\u8c61\u4ee5\u7528\u4e8e\u540e\u5411\u4f20\u9012.\n        \"\"\"\n        ctx.save_for_backward(input)\n        return input.clamp(min=0)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        \"\"\"\n        \u5728\u540e\u5411\u4f20\u9012\u4e2d,\u6211\u4eec\u6536\u5230\u4e00\u4e2a\u5f20\u91cf,\u5176\u4e2d\u5305\u542b\u76f8\u5bf9\u4e8e\u8f93\u51fa\u7684\u635f\u5931\u68af\u5ea6,\n        \u6211\u4eec\u9700\u8981\u8ba1\u7b97\u76f8\u5bf9\u4e8e\u8f93\u5165\u7684\u635f\u5931\u68af\u5ea6.\n        \"\"\"\n        input, = ctx.saved_tensors\n        grad_input = grad_output.clone()\n        grad_input[input < 0] = 0\n        return grad_input\n\n\ndtype = torch.FloatTensor\n# dtype = torch.cuda.FloatTensor # \u53d6\u6d88\u6ce8\u91ca\u4ee5\u5728GPU\u4e0a\u8fd0\u884c\n\n# N \u6279\u91cf\u5927\u5c0f; D_in\u662f\u8f93\u5165\u5c3a\u5bf8;\n# H\u662f\u9690\u85cf\u5c3a\u5bf8; D_out\u662f\u8f93\u51fa\u5c3a\u5bf8.\nN, D_in, H, D_out = 64, 1000, 100, 10\n\n# \u521b\u5efa\u968f\u673a\u5f20\u91cf\u6765\u4fdd\u5b58\u8f93\u5165\u548c\u8f93\u51fa,\u5e76\u5c06\u5b83\u4eec\u5305\u88c5\u5728\u53d8\u91cf\u4e2d.\nx = Variable(torch.randn(N, D_in).type(dtype), requires_grad=False)\ny = Variable(torch.randn(N, D_out).type(dtype), requires_grad=False)\n\n# \u4e3a\u6743\u91cd\u521b\u5efa\u968f\u673a\u5f20\u91cf,\u5e76\u5c06\u5176\u5305\u88c5\u5728\u53d8\u91cf\u4e2d.\nw1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=True)\nw2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=True)\n\nlearning_rate = 1e-6\nfor t in range(500):\n    # \u4e3a\u4e86\u5e94\u7528\u6211\u4eec\u7684\u51fd\u6570,\u6211\u4eec\u4f7f\u7528Function.apply\u65b9\u6cd5.\u6211\u4eec\u628a\u5b83\u79f0\u4e3a'relu'.\n    relu = MyReLU.apply\n\n    # \u6b63\u5411\u4f20\u9012:\u4f7f\u7528\u53d8\u91cf\u4e0a\u7684\u8fd0\u7b97\u6765\u8ba1\u7b97\u9884\u6d4b\u7684y; \n    # \u6211\u4eec\u4f7f\u7528\u6211\u4eec\u7684\u81ea\u5b9a\u4e49autograd\u64cd\u4f5c\u6765\u8ba1\u7b97ReLU.\n    y_pred = relu(x.mm(w1)).mm(w2)\n\n    # \u8ba1\u7b97\u548c\u6253\u5370\u635f\u5931\n    loss = (y_pred - y).pow(2).sum()\n    print(t, loss.data[0])\n\n    # \u4f7f\u7528autograd\u6765\u8ba1\u7b97\u53cd\u5411\u4f20\u9012.\n    loss.backward()\n\n    # \u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u66f4\u65b0\u6743\u91cd\n    w1.data -= learning_rate * w1.grad.data\n    w2.data -= learning_rate * w2.grad.data\n\n    # \u66f4\u65b0\u6743\u91cd\u540e\u624b\u52a8\u5c06\u68af\u5ea6\u5f52\u96f6\n    w1.grad.data.zero_()\n    w2.grad.data.zero_()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}