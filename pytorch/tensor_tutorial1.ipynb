{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Tensors\n",
    "=======\n",
    "\n",
    "Tensors 在 PyTorch 中的操作方式 与 Torch 几乎完全相同.\n",
    "\n",
    "用未初始化的内存创建一个大小为 (5 x 7) 的 tensor:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.FloatTensor(5, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.00000e-38 *\n",
       "       [[ 0.5786,  0.8449,  0.5327,  0.8908,  1.0286,  0.8908,  0.8908],\n",
       "        [ 1.0194,  0.9184,  0.8449,  0.9643,  0.8449,  0.9643,  0.9276],\n",
       "        [ 1.0286,  0.9092,  0.8908,  0.9276,  0.8449,  1.0194,  0.9092],\n",
       "        [ 0.8449,  1.0102,  1.0745,  0.9643,  1.0561,  0.9092,  1.0102],\n",
       "        [ 0.9276,  1.0653,  1.0286,  1.0469,  1.0010,  1.0653,  1.0469]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用 mean=0, var=1 的正态分布随机初始化一个tensor:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.6968,  1.0118, -1.3871,  0.9438,  1.3532,  0.5870, -0.8632],\n",
      "        [-1.0935,  0.0820, -1.3389, -1.7860, -0.2920, -2.1959,  0.6514],\n",
      "        [ 0.7380,  0.4910,  1.4724,  0.6694, -1.5204,  1.1456, -0.1136],\n",
      "        [ 0.0470,  0.8175, -0.7787, -2.1541,  0.6594,  0.3566,  1.0292],\n",
      "        [-2.4749,  0.9229,  0.3409,  1.3305,  0.0615,  1.3712,  0.7016]])\n",
      "torch.Size([5, 7])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(5, 7)\n",
    "print(a)\n",
    "print(a.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>Note</h4><p>``torch.Size`` 实际上是一个 tuple, 因此它支持相同的操作</p></div>\n",
    "\n",
    "Inplace / Out-of-place\n",
    "----------------------\n",
    "\n",
    "第一个不同点在于 tensor 上的所有操作, 如果想要在 tensor 自身上进行的操作\n",
    "(in-place) 就要加上一个 ``_`` 作为后缀. 例如, ``add`` 是一个 out-of-place\n",
    "的 version ,而 ``add_`` 是一个 in-place 的 version .\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],\n",
      "        [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],\n",
      "        [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],\n",
      "        [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],\n",
      "        [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000]]) tensor([[ 7.5000,  7.5000,  7.5000,  7.5000,  7.5000,  7.5000,  7.5000],\n",
      "        [ 7.5000,  7.5000,  7.5000,  7.5000,  7.5000,  7.5000,  7.5000],\n",
      "        [ 7.5000,  7.5000,  7.5000,  7.5000,  7.5000,  7.5000,  7.5000],\n",
      "        [ 7.5000,  7.5000,  7.5000,  7.5000,  7.5000,  7.5000,  7.5000],\n",
      "        [ 7.5000,  7.5000,  7.5000,  7.5000,  7.5000,  7.5000,  7.5000]])\n"
     ]
    }
   ],
   "source": [
    "a.fill_(3.5)\n",
    "# a 的值现在变为 3.5\n",
    "\n",
    "b = a.add(4.0)\n",
    "# a 的值仍然是 3.5\n",
    "# 返回的值 3.5 + 4.0 = 7.5 将作为b的值. \n",
    "\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还有一些像 ``narrow`` 的操作是没有 in-place version , 所以也就不存在 ``.narrow_``\n",
    ". 同样的, 也有像 ``fill_`` 的一些操作没有 out-of-place version . 因此, ``.fill``\n",
    "也同样不存在.\n",
    "\n",
    "Zero Indexing (零索引)\n",
    "------------------------\n",
    "\n",
    "Tensors 是 zero-indexed (索引从零开始)这是另外一个不同点. (在 lua 中, tensors 是\n",
    "one-indexed (索引从一开始))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a[0, 3]  # 从 a 中选择第一行第四列的值."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors 也可以用 Python 的切片索引\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a[:, 3:5]  # 从 a 中选择所有行中第四列和第五列的值."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No camel casing\n",
    "---------------\n",
    "\n",
    "接下来一个小的不同是所有的函数都不是 camelCase 了. \n",
    "例如 ``indexAdd`` 现在被称为 ``index_add_``\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  1.,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(5, 5)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  10.,  100.],\n",
      "        [  10.,  100.],\n",
      "        [  10.,  100.],\n",
      "        [  10.,  100.],\n",
      "        [  10.,  100.]])\n"
     ]
    }
   ],
   "source": [
    "z = torch.Tensor(5, 2)\n",
    "z[:, 0] = 10\n",
    "z[:, 1] = 100\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  10.,  100.],\n",
      "        [  10.,  100.],\n",
      "        [  10.,  100.],\n",
      "        [  10.,  100.],\n",
      "        [  10.,  100.]]) \n",
      " tensor([[ 101.,    1.,    1.,    1.,   11.],\n",
      "        [ 101.,    1.,    1.,    1.,   11.],\n",
      "        [ 101.,    1.,    1.,    1.,   11.],\n",
      "        [ 101.,    1.,    1.,    1.,   11.],\n",
      "        [ 101.,    1.,    1.,    1.,   11.]])\n"
     ]
    }
   ],
   "source": [
    "print(z ,'\\n',x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  2.,   3.,   4.],\n",
       "        [  1.,   1.,   1.],\n",
       "        [  8.,   9.,  10.],\n",
       "        [  1.,   1.,   1.],\n",
       "        [  5.,   6.,   7.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    >>> xtt = torch.ones(5, 3)\n",
    "    >>> ttt = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float)\n",
    "    >>> indextt = torch.tensor([0, 4, 2])\n",
    "    >>> xtt.index_add_(0, indextt, ttt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 601.,    1.,    1.,    1.,   61.],\n",
       "        [ 601.,    1.,    1.,    1.,   61.],\n",
       "        [ 601.,    1.,    1.,    1.,   61.],\n",
       "        [ 601.,    1.,    1.,    1.,   61.],\n",
       "        [ 601.,    1.,    1.,    1.,   61.]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.index_add_(1, torch.LongTensor([4, 0]), z)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 801.,  201.,  201.,  201.,  261.],\n",
      "        [ 601.,    1.,    1.,    1.,   61.],\n",
      "        [ 601.,    1.,    1.,    1.,   61.],\n",
      "        [ 611.,   11.,   11.,   11.,   71.],\n",
      "        [ 611.,   11.,   11.,   11.,   71.]])\n"
     ]
    }
   ],
   "source": [
    "x.index_add_(0, torch.LongTensor([4, 0]), z.transpose(0,1))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy Bridge\n",
    "------------\n",
    "\n",
    "将 torch Tensor 转换为一个 numpy array, 反之亦然.\n",
    "Torch Tensor 和 numpy array 将会共用底层的内存,\n",
    "改变其中一个, 另外一个也会随之改变.\n",
    "\n",
    "将 torch Tensor 转换为 numpy Array\n",
    "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.,  1.,  1.,  1.,  1.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "b = a.numpy()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3.,  3.,  3.,  3.,  3.])\n",
      "[3. 3. 3. 3. 3.]\n"
     ]
    }
   ],
   "source": [
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将 numpy Array 转换为 torch Tensor\n",
    "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2.]\n",
      "tensor([ 2.,  2.,  2.,  2.,  2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "print(a)\n",
    "print(b)  # 看一下通过改变 np array 来自动的改变 torch Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了 CharTensor 之外, 所有 CPU 上的 Tensors 支持转变为 NumPy 并且\n",
    "转换回来. \n",
    "\n",
    "CUDA Tensors\n",
    "------------\n",
    "\n",
    "CUDA Tensors 在 pytorch 中非常好用, 并且一个 CUDA tensor\n",
    "从 CPU 转换到 GPU 仍将保持它底层的类型.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 让我们在 CUDA 可用的时候运行这个单元\n",
    "if torch.cuda.is_available():\n",
    "    # 创建一个 LongTensor 并且将其转换使用 GPU\n",
    "    # 的 torch.cuda.LongTensor 类型\n",
    "    a = torch.LongTensor(10).fill_(3).cuda()\n",
    "    print(type(a))\n",
    "    b = a.cpu()\n",
    "    # 将它转换到 CPU\n",
    "    # 类型变回 torch.LongTensor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
