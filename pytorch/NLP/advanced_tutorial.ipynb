{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\u9ad8\u7ea7\u6559\u7a0b: \u4f5c\u51fa\u52a8\u6001\u51b3\u7b56\u548c Bi-LSTM CRF\n======================================================\n\n\u52a8\u6001 VS \u9759\u6001\u6df1\u5ea6\u5b66\u4e60\u5de5\u5177\u96c6\n--------------------------------------------\n\nPytorch \u662f\u4e00\u4e2a *\u52a8\u6001* \u795e\u7ecf\u7f51\u7edc\u5de5\u5177\u5305. \u53e6\u4e00\u4e2a\u52a8\u6001\u5de5\u5177\u5305\u7684\u4f8b\u5b50\u662f `Dynet <https://github.com/clab/dynet>`__ \n(\u6211\u4e4b\u6240\u4ee5\u63d0\u8fd9\u4e2a\u662f\u56e0\u4e3a\u4f7f\u7528 Pytorch \u548c Dynet \u662f\u5341\u5206\u7c7b\u4f3c\u7684. \n\u5982\u679c\u4f60\u770b\u8fc7 Dynet \u4e2d\u7684\u4f8b\u5b50, \u90a3\u4e48\u5b83\u5c06\u6709\u53ef\u80fd\u5bf9\u4f60\u5728 Pytorch \u4e0b\u5b9e\u73b0\u5b83\u6709\u5e2e\u52a9). \u4e0e\u52a8\u6001\u76f8\u53cd\u7684\u662f *\u9759\u6001* \u5de5\u5177\u5305, \n\u5305\u62ec\u4e86 Theano, Keras, TensorFlow \u7b49\u7b49. \n\u4e0b\u9762\u662f\u8fd9\u4e24\u8005\u6838\u5fc3\u7684\u4e00\u4e9b\u533a\u522b: \n\n* \u5728\u4e00\u4e2a\u9759\u6001\u5de5\u5177\u5305\u4e2d, \u4f60\u4e00\u6b21\u6027\u5b9a\u4e49\u597d\u4e00\u4e2a\u8ba1\u7b97\u56fe, \u63a5\u7740\u7f16\u8bd1\u5b83, \u7136\u540e\u628a\u6570\u636e\u6d41\u8f93\u5b9e\u4f8b\u9001\u8fdb\u53bb. \n* \u5728\u4e00\u4e2a\u52a8\u6001\u5de5\u5177\u5305\u4e2d, \u4f60 *\u4e3a\u6bcf\u4e00\u4e2a\u5b9e\u4f8b* \u5b9a\u4e49\u4e00\u4e2a\u8ba1\u7b97\u56fe, \u5b83\u5b8c\u5168\u4e0d\u9700\u8981\u88ab\u7f16\u8bd1\u5e76\u4e14\u662f\u5728\u8fd0\u884c\u4e2d\u5b9e\u65f6\u6267\u884c\u7684. \n\n\u82e5\u6ca1\u6709\u4e30\u5bcc\u7684\u7ecf\u9a8c, \u4f60\u5f88\u96be\u4f53\u4f1a\u51fa\u5176\u4e2d\u7684\u5dee\u522b. \u4e3e\u4e00\u4e2a\u4f8b\u5b50, \u5047\u8bbe\u6211\u4eec\u60f3\u8981\u6784\u5efa\u4e00\u4e2a\u6df1\u5ea6\u53e5\u6cd5\u5206\u6790\u5668. \n\u90a3\u4e48\u6211\u4eec\u7684\u6a21\u578b\u9700\u8981\u4e0b\u5217\u7684\u4e00\u4e9b\u6b65\u9aa4: \n\n* \u6211\u4eec\u4ece\u4e0b\u5f80\u4e0a\u6784\u5efa\u6811\n* \u6807\u6ce8\u6839\u8282\u70b9(\u53e5\u5b50\u4e2d\u7684\u8bcd\u8bed)\n* \u4ece\u90a3\u513f\u5f00\u59cb, \u4f7f\u7528\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc\u548c\u8bcd\u5411\u91cf\u6765\u627e\u5230\u7ec4\u6210\u53e5\u6cd5\u7684\u4e0d\u540c\u7ec4\u5408. \u4e00\u65e6\u5f53\u4f60\u5f62\u6210\u4e86\u4e00\u4e2a\u65b0\u7684\u53e5\u6cd5, \n  \u4f7f\u7528\u67d0\u79cd\u65b9\u5f0f\u5f97\u5230\u53e5\u6cd5\u7684\u5d4c\u5165\u8868\u793a (embedding). \u5728\u8fd9\u4e2a\u4f8b\u5b50\u91cc, \u6211\u4eec\u7684\u7f51\u7edc\u67b6\u6784\u5c06\u4f1a\n  \u5b8c\u5168\u7684\u4f9d\u8d56\u4e8e\u8f93\u5165\u7684\u53e5\u5b50. \u6765\u770b\u8fd9\u4e2a\u53e5\u5b50: \"\u7eff\u8272\u732b\u6293\u4e86\u5899\", \u5728\u8fd9\u4e2a\u6a21\u578b\u7684\u67d0\u4e00\u8282\u70b9, \u6211\u4eec\u60f3\u8981\u628a\u8303\u56f4\n  $(i,j,r) = (1, 3, \\text{NP})$ \u5408\u5e76\u8d77\u6765(\u5373, \u4e00\u4e2a NP \u53e5\u6cd5\u8303\u56f4\u8de8\u8d8a\u8bcd1\u5230\u8bcd3, \n  \u5728\u8fd9\u4e2a\u4f8b\u5b50\u4e2d\u662f\"\u7eff\u8272\u732b\"). \n\n\u7136\u800c, \u53e6\u4e00\u4e2a\u53e5\u5b50\u53ef\u80fd\u662f\"\u67d0\u5904, \u90a3\u4e2a\u5927\u80a5\u732b\u6293\u4e86\u5899.\" \u5728\u8fd9\u4e2a\u53e5\u5b50\u4e2d, \n\u6211\u4eec\u60f3\u8981\u5728\u67d0\u70b9\u5f62\u6210\u53e5\u6cd5 $(2, 4, NP)$ . \u6211\u4eec\u60f3\u8981\u5f62\u6210\u7684\u53e5\u6cd5\u5c06\u4f1a\u4f9d\u8d56\u4e8e\u8fd9\u4e2a\u5b9e\u4f8b. \u5982\u679c\u4ec5\u4ec5\u7f16\u8bd1\u8fd9\u4e2a\u8ba1\u7b97\u56fe\u4e00\u6b21, \n\u5c31\u50cf\u5728\u9759\u6001\u5de5\u5177\u5305\u4e2d\u90a3\u6837, \u90a3\u4e48\u6211\u4eec\u7ed9\u8fd9\u4e2a\u903b\u8f91\u7f16\u7a0b\u5c06\u4f1a\u53d8\u5f97\u5341\u5206\u56f0\u96be\u6216\u8005\u6839\u672c\u4e0d\u53ef\u80fd. \u7136\u800c, \u5728\u4e00\u4e2a\u52a8\u6001\u5de5\u5177\u5305\u4e2d, \n\u5e76\u4e0d\u4ec5\u4ec5\u53ea\u6709\u4e00\u4e2a\u9884\u5b9a\u4e49\u7684\u8ba1\u7b97\u56fe. \u5bf9\u4e8e\u6bcf\u4e00\u4e2a\u5b9e\u4f8b, \u90fd\u80fd\u591f\u6709\u4e00\u4e2a\u65b0\u7684\u8ba1\u7b97\u56fe, \u6240\u4ee5\u4e0a\u9762\u7684\u95ee\u9898\u5c31\u4e0d\u590d\u5b58\u5728\u4e86. \n\n\u52a8\u6001\u5de5\u5177\u5305\u4e5f\u5177\u6709\u66f4\u5bb9\u6613\u8c03\u8bd5\u548c\u66f4\u63a5\u8fd1\u6240\u4f7f\u7528\u7684\u7f16\u7a0b\u8bed\u8a00\u7684\u7279\u70b9(\u6211\u7684\u610f\u601d\u662f Pytorch \u548c Dynet \u770b\u4e0a\u53bb\n\u6bd4 Keras \u548c Theano \u66f4\u50cf Python). \n\n\nBi-LSTM CRF (\u6761\u4ef6\u968f\u673a\u573a) \u8ba8\u8bba\n-------------------------------------------\n\n\u5728\u8fd9\u4e00\u90e8\u5206, \u6211\u4eec\u5c06\u4f1a\u770b\u5230\u4e00\u4e2a\u5b8c\u6574\u4e14\u590d\u6742\u7684 Bi-LSTM CRF (\u6761\u4ef6\u968f\u673a\u573a)\u7528\u6765\u547d\u540d\u5b9e\u4f53\u8bc6\u522b (NER) \u7684\u4f8b\u5b50. \n\u4e0a\u9762\u7684 LSTM \u6807\u6ce8\u5de5\u5177\u901a\u5e38\u60c5\u51b5\u4e0b\u5bf9\u8bcd\u6027\u6807\u6ce8\u5df2\u7ecf\u8db3\u591f\u7528\u4e86, \u4f46\u4e00\u4e2a\u5e8f\u5217\u6a21\u578b\u6bd4\u5982 CRF \u5bf9\u4e8e\u5728 NER \u4e0b\u53d6\u5f97\n\u5f3a\u52b2\u7684\u8868\u73b0\u662f\u81f3\u5173\u91cd\u8981\u7684. \u5047\u8bbe\u719f\u6089 CRF. \u5c3d\u7ba1\u8fd9\u4e2a\u540d\u5b57\u542c\u4e0a\u53bb\u5413\u4eba, \u4f46\u6240\u6709\u7684\u6a21\u578b\u53ea\u662f\u4e00\u4e2a\u7531 LSTM \u63d0\u4f9b\n\u7279\u5f81\u7684 CRF. \u4f46\u8fd9\u662f\u4e00\u4e2a\u9ad8\u7ea7\u7684\u6a21\u578b, \u8fdc\u6bd4\u8fd9\u4e2a\u6559\u7a0b\u4e2d\u7684\u5176\u5b83\u65e9\u671f\u7684\u6a21\u578b\u66f4\u52a0\u590d\u6742. \u5982\u679c\u4f60\u8981\u8df3\u8fc7\u8fd9\u4e00\u90e8\u5206, \n\u6ca1\u6709\u5173\u7cfb. \u60f3\u8981\u786e\u5b9a\u4f60\u662f\u5426\u51c6\u5907\u597d, \u90a3\u770b\u770b\u4f60\u662f\u4e0d\u662f\u80fd\u591f: \n\n-  \u590d\u73b0\u6807\u7b7e k \u7684\u7b2c i \u6b65\u7ef4\u7279\u6bd4\u53d8\u91cf\u7684\u7b97\u6cd5. \n-  \u4fee\u6539\u4e0a\u8ff0\u5faa\u73af\u6765\u8ba1\u7b97\u6b63\u5411\u53d8\u91cf. \n-  \u518d\u4e00\u6b21\u4fee\u6539\u4e0a\u8ff0\u590d\u73b0\u6765\u5728\u5bf9\u6570\u7a7a\u95f4\u4e2d\u8ba1\u7b97\u6b63\u5411\u53d8\u91cf. (\u63d0\u793a: \u5bf9\u6570-\u6c42\u548c-\u6307\u6570)\n\n\u5982\u679c\u4f60\u80fd\u591f\u5b8c\u6210\u4ee5\u4e0a\u4e09\u4ef6\u4e8b, \u90a3\u4e48\u4f60\u5c31\u4e0d\u96be\u7406\u89e3\u4e0b\u9762\u7684\u4ee3\u7801\u4e86. \u56de\u60f3\u4e00\u4e0b, CRF \u8ba1\u7b97\u7684\u662f\u4e00\u4e2a\u6761\u4ef6\u6982\u7387. \n\u8ba9 $y$ \u4f5c\u4e3a\u4e00\u4e2a\u6807\u6ce8\u5e8f\u5217, $x$ \u4f5c\u4e3a\u67d0\u4e2a\u8bcd\u7684\u8f93\u5165\u5e8f\u5217. \u63a5\u4e0b\u6765\u6211\u4eec\u8ba1\u7b97: \n\n\\begin{align}P(y|x) = \\frac{\\exp{(\\text{Score}(x, y)})}{\\sum_{y'} \\exp{(\\text{Score}(x, y')})}\\end{align}\n\n\u4e0a\u9762\u7684\u5206\u6570 Score \u662f\u7531\u5b9a\u4e49\u4e00\u4e9b\u5bf9\u6570\u52bf\u80fd \n$\\log \\psi_i(x,y)$ \u800c\u51b3\u5b9a\u7684. \u8fdb\u800c\n\n\\begin{align}\\text{Score}(x,y) = \\sum_i \\log \\psi_i(x,y)\\end{align}\n\n\u8981\u4f7f\u5206\u5272\u51fd\u6570\u6613\u4e8e\u638c\u63a7, \u52bf\u80fd\u5fc5\u987b\u53ea\u80fd\u96c6\u4e2d\u4e8e\u5c40\u90e8\u7684\u7279\u5f81. \n\n\n\u5728 Bi-LSTM CRF \u4e2d, \u6211\u4eec\u5b9a\u4e49\u4e24\u79cd\u52bf\u80fd (potential): \u91ca\u653e (emission) \u548c\u8fc7\u6e21 (transition). \n\u7d22\u5f15 $i$ \u5904\u5b57\u7684\u91ca\u653e\u52bf\u80fd\u6765\u81ea\u4e8e $i$ \u65f6\u95f4\u5904\u7684 Bi-LSTM \u7684\u9690\u85cf\u72b6\u6001. \n\u8fc7\u6e21\u52bf\u80fd\u7684\u5206\u6570\u50a8\u5b58\u5728 $|T|x|T|$ \u77e9\u9635 $\\textbf{P}$ , \u5176\u4e2d \n$T$ \u662f\u6807\u6ce8\u96c6\u5408. \u5728\u6211\u7684\u5b9e\u73b0\u4e2d, $\\textbf{P}_{j,k}$ \u662f\u4ece\u6807\u6ce8 $k$ \u8fc7\u6e21\u5230\n\u6807\u6ce8 $j$ \u7684\u5f97\u5206. \u56e0\u6b64: \n\n\\begin{align}\\text{Score}(x,y) = \\sum_i \\log \\psi_\\text{EMIT}(y_i \\rightarrow x_i) + \\log \\psi_\\text{TRANS}(y_{i-1} \\rightarrow y_i)\\end{align}\n\n\\begin{align}= \\sum_i h_i[y_i] + \\textbf{P}_{y_i, y_{i-1}}\\end{align}\n\n\u5728\u4e0a\u9762\u7b2c\u4e8c\u4e2a\u8868\u8fbe\u5f0f\u4e2d, \u6211\u4eec\u8ba4\u4e3a\u6807\u7b7e\u88ab\u5206\u914d\u4e86\u72ec\u4e00\u65e0\u4e8c\u7684\u975e\u8d1f\u7d22\u5f15. \n\n\u5982\u679c\u4e0a\u9762\u7684\u8ba8\u8bba\u592a\u7b80\u77ed\u4e86, \u4f60\u8fd8\u53ef\u4ee5\u770b\u770b `\u8fd9\u4e2a <http://www.cs.columbia.edu/%7Emcollins/crf.pdf>`__ \n\u7531 Michael Collins \u5199\u7684\u5173\u4e8e CRFs \u7684\u6587\u7ae0. \n\n\u5177\u4f53\u5b9e\u73b0\u7b14\u8bb0\n--------------------\n\n\u4e0b\u9762\u7684\u4f8b\u5b50\u5b9e\u73b0\u4e86\u5728\u5bf9\u6570\u7a7a\u95f4\u4e2d\u7684\u524d\u5411\u7b97\u6cd5\u6765\u8ba1\u7b97\u51fa\u5206\u5272\u51fd\u6570\u548c\u7ef4\u7279\u6bd4\u7b97\u6cd5\u6765\u8fdb\u884c\u8bd1\u7801. \n\u53cd\u5411\u4f20\u64ad\u5c06\u4f1a\u4e3a\u6211\u4eec\u81ea\u52a8\u8ba1\u7b97\u51fa\u68af\u5ea6. \u6211\u4eec\u4e0d\u9700\u8981\u624b\u52a8\u53bb\u5b9e\u73b0\u8fd9\u4e2a. \n\n\u8fd9\u4e2a\u4ee3\u7801\u4e2d\u7684\u5b9e\u73b0\u5e76\u6ca1\u6709\u4f18\u5316\u8fc7. \u5982\u679c\u4f60\u7406\u89e3\u4e0b\u9762\u7684\u8fc7\u7a0b, \u4e5f\u8bb8\u4f60\u4f1a\u89c9\u5f97\u4e0b\u9762\u7684\u4ee3\u7801\u4e2d, \u524d\u5411\u7b97\u6cd5\u4e2d\n\u7684\u8fed\u4ee3\u4e0b\u4e00\u6b21\u6807\u6ce8\u53ef\u4ee5\u5728\u4e00\u6b21\u5927\u7684\u8fd0\u7b97\u4e2d\u5b8c\u6210. \u867d\u7136\u6709\u7b80\u5316\u7684\u4f59\u5730, \u4f46\u6211\u60f3\u7684\u662f\u8ba9\u4ee3\u7801\u53ef\u8bfb\u6027\u66f4\u597d. \n\u5982\u679c\u4f60\u60f3\u8fdb\u884c\u76f8\u5173\u7684\u4fee\u6539, \u4e5f\u8bb8\u4f60\u53ef\u4ee5\u5728\u4e00\u4e9b\u771f\u5b9e\u7684\u4efb\u52a1\u4e2d\u4f7f\u7528\u8fd9\u4e2a\u6807\u6ce8\u5668. \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# \u4f5c\u8005: Robert Guthrie\n\nimport torch\nimport torch.autograd as autograd\nimport torch.nn as nn\nimport torch.optim as optim\n\ntorch.manual_seed(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u4e00\u4e9b\u5e2e\u52a9\u51fd\u6570, \u4f7f\u4ee3\u7801\u53ef\u8bfb\u6027\u66f4\u597d\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def to_scalar(var):\n    # \u8fd4\u56de python \u6d6e\u70b9\u6570 (float)\n    return var.view(-1).data.tolist()[0]\n\n\ndef argmax(vec):\n    # \u4ee5 python \u6574\u6570\u7684\u5f62\u5f0f\u8fd4\u56de argmax\n    _, idx = torch.max(vec, 1)\n    return to_scalar(idx)\n\n\ndef prepare_sequence(seq, to_ix):\n    idxs = [to_ix[w] for w in seq]\n    tensor = torch.LongTensor(idxs)\n    return autograd.Variable(tensor)\n\n\n# \u4f7f\u7528\u6570\u503c\u4e0a\u7a33\u5b9a\u7684\u65b9\u6cd5\u4e3a\u524d\u5411\u7b97\u6cd5\u8ba1\u7b97\u6307\u6570\u548c\u7684\u5bf9\u6570\ndef log_sum_exp(vec):\n    max_score = vec[0, argmax(vec)]\n    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n    return max_score + \\\n        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u521b\u5efa\u6a21\u578b\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class BiLSTM_CRF(nn.Module):\n\n    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n        super(BiLSTM_CRF, self).__init__()\n        self.embedding_dim = embedding_dim\n        self.hidden_dim = hidden_dim\n        self.vocab_size = vocab_size\n        self.tag_to_ix = tag_to_ix\n        self.tagset_size = len(tag_to_ix)\n\n        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n                            num_layers=1, bidirectional=True)\n\n        # \u5c06LSTM\u7684\u8f93\u51fa\u6620\u5c04\u5230\u6807\u8bb0\u7a7a\u95f4\n        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n\n        # \u8fc7\u6e21\u53c2\u6570\u77e9\u9635. \u6761\u76ee i,j \u662f\n        # *\u4ece* j *\u5230* i \u7684\u8fc7\u6e21\u7684\u5206\u6570\n        self.transitions = nn.Parameter(\n            torch.randn(self.tagset_size, self.tagset_size))\n\n        # \u8fd9\u4e24\u53e5\u58f0\u660e\u5f3a\u5236\u7ea6\u675f\u4e86\u6211\u4eec\u4e0d\u80fd\n        # \u5411\u5f00\u59cb\u6807\u8bb0\u6807\u6ce8\u4f20\u9012\u548c\u4ece\u7ed3\u675f\u6807\u6ce8\u4f20\u9012\n        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n\n        self.hidden = self.init_hidden()\n\n    def init_hidden(self):\n        return (autograd.Variable(torch.randn(2, 1, self.hidden_dim // 2)),\n                autograd.Variable(torch.randn(2, 1, self.hidden_dim // 2)))\n\n    def _forward_alg(self, feats):\n        # \u6267\u884c\u524d\u5411\u7b97\u6cd5\u6765\u8ba1\u7b97\u5206\u5272\u51fd\u6570\n        init_alphas = torch.Tensor(1, self.tagset_size).fill_(-10000.)\n        # START_TAG \u5305\u542b\u6240\u6709\u7684\u5206\u6570\n        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n\n        # \u5c06\u5176\u5305\u5728\u4e00\u4e2a\u53d8\u91cf\u7c7b\u578b\u4e2d\u7ee7\u800c\u5f97\u5230\u81ea\u52a8\u7684\u53cd\u5411\u4f20\u64ad\n        forward_var = autograd.Variable(init_alphas)\n\n        # \u5728\u53e5\u5b50\u4e2d\u8fed\u4ee3\n        for feat in feats:\n            alphas_t = []  # \u5728\u8fd9\u4e2a\u65f6\u95f4\u6b65\u7684\u524d\u5411\u53d8\u91cf\n            for next_tag in range(self.tagset_size):\n                # \u5bf9 emission \u5f97\u5206\u6267\u884c\u5e7f\u64ad\u673a\u5236: \u5b83\u603b\u662f\u76f8\u540c\u7684, \n                # \u4e0d\u8bba\u524d\u4e00\u4e2a\u6807\u6ce8\u5982\u4f55\n                emit_score = feat[next_tag].view(\n                    1, -1).expand(1, self.tagset_size)\n                # trans_score \u7b2c i \u4e2a\u6761\u76ee\u662f\n                # \u4ecei\u8fc7\u6e21\u5230 next_tag \u7684\u5206\u6570\n                trans_score = self.transitions[next_tag].view(1, -1)\n                # next_tag_var \u7b2c i \u4e2a\u6761\u76ee\u662f\u5728\u6211\u4eec\u6267\u884c \u5bf9\u6570-\u6c42\u548c-\u6307\u6570 \u524d\n                # \u8fb9\u7f18\u7684\u503c (i -> next_tag)\n                next_tag_var = forward_var + trans_score + emit_score\n                # \u8fd9\u4e2a\u6807\u6ce8\u7684\u524d\u5411\u53d8\u91cf\u662f\n                # \u5bf9\u6240\u6709\u7684\u5206\u6570\u6267\u884c \u5bf9\u6570-\u6c42\u548c-\u6307\u6570\n                alphas_t.append(log_sum_exp(next_tag_var))\n            forward_var = torch.cat(alphas_t).view(1, -1)\n        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n        alpha = log_sum_exp(terminal_var)\n        return alpha\n\n    def _get_lstm_features(self, sentence):\n        self.hidden = self.init_hidden()\n        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n        lstm_feats = self.hidden2tag(lstm_out)\n        return lstm_feats\n\n    def _score_sentence(self, feats, tags):\n        # \u7ed9\u51fa\u6807\u8bb0\u5e8f\u5217\u7684\u5206\u6570\n        score = autograd.Variable(torch.Tensor([0]))\n        tags = torch.cat([torch.LongTensor([self.tag_to_ix[START_TAG]]), tags])\n        for i, feat in enumerate(feats):\n            score = score + \\\n                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n        return score\n\n    def _viterbi_decode(self, feats):\n        backpointers = []\n\n        # \u5728\u5bf9\u6570\u7a7a\u95f4\u4e2d\u521d\u59cb\u5316\u7ef4\u7279\u6bd4\u53d8\u91cf\n        init_vvars = torch.Tensor(1, self.tagset_size).fill_(-10000.)\n        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n\n        # \u5728\u7b2c i \u6b65\u7684 forward_var \u5b58\u653e\u7b2c i-1 \u6b65\u7684\u7ef4\u7279\u6bd4\u53d8\u91cf\n        forward_var = autograd.Variable(init_vvars)\n        for feat in feats:\n            bptrs_t = []  # \u5b58\u653e\u8fd9\u4e00\u6b65\u7684\u540e\u6307\u9488\n            viterbivars_t = []  # \u5b58\u653e\u8fd9\u4e00\u6b65\u7684\u7ef4\u7279\u6bd4\u53d8\u91cf\n\n            for next_tag in range(self.tagset_size):\n                # next_tag_var[i] \u5b58\u653e\u5148\u524d\u4e00\u6b65\u6807\u6ce8i\u7684\n                # \u7ef4\u7279\u6bd4\u53d8\u91cf, \u52a0\u4e0a\u4e86\u4ece\u6807\u6ce8 i \u5230 next_tag \u7684\u8fc7\u6e21\n                # \u7684\u5206\u6570\n                # \u6211\u4eec\u5728\u8fd9\u91cc\u5e76\u6ca1\u6709\u5c06 emission \u5206\u6570\u5305\u542b\u8fdb\u6765, \u56e0\u4e3a\n                # \u6700\u5927\u503c\u5e76\u4e0d\u4f9d\u8d56\u4e8e\u5b83\u4eec(\u6211\u4eec\u5728\u4e0b\u9762\u5bf9\u5b83\u4eec\u8fdb\u884c\u7684\u662f\u76f8\u52a0)\n                next_tag_var = forward_var + self.transitions[next_tag]\n                best_tag_id = argmax(next_tag_var)\n                bptrs_t.append(best_tag_id)\n                viterbivars_t.append(next_tag_var[0][best_tag_id])\n            # \u73b0\u5728\u5c06\u6240\u6709 emission \u5f97\u5206\u76f8\u52a0, \u5c06 forward_var \n            # \u8d4b\u503c\u5230\u6211\u4eec\u521a\u521a\u8ba1\u7b97\u51fa\u6765\u7684\u7ef4\u7279\u6bd4\u53d8\u91cf\u96c6\u5408\n            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n            backpointers.append(bptrs_t)\n\n        # \u8fc7\u6e21\u5230 STOP_TAG\n        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n        best_tag_id = argmax(terminal_var)\n        path_score = terminal_var[0][best_tag_id]\n\n        # \u8ddf\u7740\u540e\u6307\u9488\u53bb\u89e3\u7801\u6700\u4f73\u8def\u5f84\n        best_path = [best_tag_id]\n        for bptrs_t in reversed(backpointers):\n            best_tag_id = bptrs_t[best_tag_id]\n            best_path.append(best_tag_id)\n        # \u5f39\u51fa\u5f00\u59cb\u7684\u6807\u7b7e (\u6211\u4eec\u5e76\u4e0d\u5e0c\u671b\u628a\u8fd9\u4e2a\u8fd4\u56de\u5230\u8c03\u7528\u51fd\u6570)\n        start = best_path.pop()\n        assert start == self.tag_to_ix[START_TAG]  # \u5065\u5168\u6027\u68c0\u67e5\n        best_path.reverse()\n        return path_score, best_path\n\n    def neg_log_likelihood(self, sentence, tags):\n        feats = self._get_lstm_features(sentence)\n        forward_score = self._forward_alg(feats)\n        gold_score = self._score_sentence(feats, tags)\n        return forward_score - gold_score\n\n    def forward(self, sentence):  # \u4e0d\u8981\u628a\u8fd9\u548c\u4e0a\u9762\u7684 _forward_alg \u6df7\u6dc6\n        # \u5f97\u5230 BiLSTM \u8f93\u51fa\u5206\u6570\n        lstm_feats = self._get_lstm_features(sentence)\n\n        # \u7ed9\u5b9a\u7279\u5f81, \u627e\u5230\u6700\u597d\u7684\u8def\u5f84\n        score, tag_seq = self._viterbi_decode(lstm_feats)\n        return score, tag_seq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u8fd0\u884c\u8bad\u7ec3\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "START_TAG = \"<START>\"\nSTOP_TAG = \"<STOP>\"\nEMBEDDING_DIM = 5\nHIDDEN_DIM = 4\n\n# \u5236\u9020\u4e00\u4e9b\u8bad\u7ec3\u6570\u636e\ntraining_data = [(\n    \"the wall street journal reported today that apple corporation made money\".split(),\n    \"B I I I O O O B I O O\".split()\n), (\n    \"georgia tech is a university in georgia\".split(),\n    \"B I O O O O B\".split()\n)]\n\nword_to_ix = {}\nfor sentence, tags in training_data:\n    for word in sentence:\n        if word not in word_to_ix:\n            word_to_ix[word] = len(word_to_ix)\n\ntag_to_ix = {\"B\": 0, \"I\": 1, \"O\": 2, START_TAG: 3, STOP_TAG: 4}\n\nmodel = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM)\noptimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n\n# \u5728\u8bad\u7ec3\u4e4b\u524d\u68c0\u67e5\u9884\u6d4b\u7ed3\u679c\nprecheck_sent = prepare_sequence(training_data[0][0], word_to_ix)\nprecheck_tags = torch.LongTensor([tag_to_ix[t] for t in training_data[0][1]])\nprint(model(precheck_sent))\n\n# \u786e\u8ba4\u4ece\u4e4b\u524d\u7684 LSTM \u90e8\u5206\u7684 prepare_sequence \u88ab\u52a0\u8f7d\u4e86\nfor epoch in range(\n        300):  # \u53c8\u4e00\u6b21, \u6b63\u5e38\u60c5\u51b5\u4e0b\u4f60\u4e0d\u4f1a\u8bad\u7ec3300\u4e2a epoch, \u8fd9\u53ea\u662f\u793a\u4f8b\u6570\u636e\n    for sentence, tags in training_data:\n        # \u7b2c\u4e00\u6b65: \u9700\u8981\u8bb0\u4f4f\u7684\u662fPytorch\u4f1a\u7d2f\u79ef\u68af\u5ea6\n        # \u6211\u4eec\u9700\u8981\u5728\u6bcf\u6b21\u5b9e\u4f8b\u4e4b\u524d\u628a\u5b83\u4eec\u6e05\u9664\n        model.zero_grad()\n\n        # \u7b2c\u4e8c\u6b65: \u4e3a\u6211\u4eec\u7684\u7f51\u7edc\u51c6\u5907\u597d\u8f93\u5165, \u5373\n        # \u628a\u5b83\u4eec\u8f6c\u53d8\u6210\u5355\u8bcd\u7d22\u5f15\u53d8\u91cf (Variables)\n        sentence_in = prepare_sequence(sentence, word_to_ix)\n        targets = torch.LongTensor([tag_to_ix[t] for t in tags])\n\n        # \u7b2c\u4e09\u6b65: \u8fd0\u884c\u524d\u5411\u4f20\u9012. \n        neg_log_likelihood = model.neg_log_likelihood(sentence_in, targets)\n\n        # \u7b2c\u56db\u6b65: \u8ba1\u7b97\u635f\u5931, \u68af\u5ea6\u4ee5\u53ca\n        # \u4f7f\u7528 optimizer.step() \u6765\u66f4\u65b0\u53c2\u6570\n        neg_log_likelihood.backward()\n        optimizer.step()\n\n# \u5728\u8bad\u7ec3\u4e4b\u540e\u68c0\u67e5\u9884\u6d4b\u7ed3\u679c\nprecheck_sent = prepare_sequence(training_data[0][0], word_to_ix)\nprint(model(precheck_sent))\n# \u6211\u4eec\u5b8c\u6210\u4e86!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u7ec3\u4e60:  \u4e3a\u533a\u522b\u6027\u6807\u6ce8\u5b9a\u4e49\u4e00\u4e2a\u65b0\u7684\u635f\u5931\u51fd\u6570\n--------------------------------------------------------\n\n\u5728\u89e3\u7801\u7684\u65f6\u5019, \u6211\u4eec\u4e0d\u4e00\u5b9a\u9700\u8981\u521b\u5efa\u4e00\u4e2a\u8ba1\u7b97\u56fe, \u56e0\u4e3a\u6211\u4eec\u5e76\u4e0d\u4ece\u7ef4\u7279\u6bd4\u8def\u5f84\u5206\u6570\u4e2d\u505a\u53cd\u5411\u4f20\u64ad. \n\u4e0d\u7ba1\u600e\u6837, \u65e2\u7136\u6211\u4eec\u6709\u4e86\u5b83, \u5c1d\u8bd5\u8bad\u7ec3\u8fd9\u4e2a\u6807\u6ce8\u5668, \u4f7f\u5176\u635f\u5931\u51fd\u6570\u662f\u7ef4\u7279\u6bd4\u8def\u5f84\u5206\u6570\u548c\u9ec4\u91d1\u6807\u51c6\u5206\u6570\u4e4b\u5dee. \n\u9700\u8981\u5f04\u6e05\u695a\u7684\u662f, \u8fd9\u4e2a\u51fd\u6570\u5728\u9884\u6d4b\u6807\u6ce8\u5e8f\u5217\u662f\u6b63\u786e\u7684\u65f6\u5019\u5e94\u5f53\u5927\u4e8e\u7b49\u4e8e0. \n\u8fd9\u672c\u8d28\u4e0a\u662f *\u7ed3\u6784\u5316\u611f\u77e5\u673a* . \n\n\u8fd9\u4e2a\u6539\u52a8\u5e94\u5f53\u662f\u5f88\u7b80\u77ed\u7684, \u56e0\u4e3a Viterbi \u548c score\\_sentence \u662f\u5df2\u7ecf\u5b9e\u73b0\u597d\u4e86\u7684. \n\u8fd9\u662f *\u4f9d\u8d56\u4e8e\u8bad\u7ec3\u5b9e\u4f8b\u7684* \u8ba1\u7b97\u56fe\u7684\u5f62\u72b6\u7684\u4e00\u4e2a\u4f8b\u5b50. \u4f46\u6211\u4eec\u5e76\u6ca1\u6709\u5c1d\u8bd5\u8fc7\u5728\u4e00\u4e2a\u9759\u6001\u5de5\u5177\u5305\u4e0a\u5b9e\u73b0\u8fc7, \n\u6211\u60f3\u8c61\u4e2d\u8fd9\u662f\u53ef\u884c\u7684\u4f46\u5e76\u4e0d\u662f\u5f88\u663e\u800c\u6613\u89c1. \n\n\u627e\u4e00\u4e9b\u771f\u5b9e\u6570\u636e\u505a\u4e00\u4e0b\u6bd4\u8f83\u5427!\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}