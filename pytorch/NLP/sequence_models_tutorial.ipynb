{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\u5e8f\u5217\u6a21\u578b\u548c LSTM \u7f51\u7edc\uff08\u957f\u77ed\u8bb0\u5fc6\u7f51\u7edc\uff09\n===================================================\n\n\u4e4b\u524d\u6211\u4eec\u5df2\u7ecf\u5b66\u8fc7\u4e86\u8bb8\u591a\u7684\u524d\u9988\u7f51\u7edc. \u6240\u8c13\u524d\u9988\u7f51\u7edc, \u5c31\u662f\u7f51\u7edc\u4e2d\u4e0d\u4f1a\u4fdd\u5b58\u72b6\u6001. \u7136\u800c\u6709\u65f6\n\u8fd9\u5e76\u4e0d\u662f\u6211\u4eec\u60f3\u8981\u7684\u6548\u679c. \u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406 (NLP, Natural Language Processing)\n\u4e2d, \u5e8f\u5217\u6a21\u578b\u662f\u4e00\u4e2a\u6838\u5fc3\u7684\u6982\u5ff5. \u6240\u8c13\u5e8f\u5217\u6a21\u578b, \u5373\u8f93\u5165\u4f9d\u8d56\u4e8e\u65f6\u95f4\u4fe1\u606f\u7684\u6a21\u578b. \u4e00\u4e2a\u5178\u578b\n\u7684\u5e8f\u5217\u6a21\u578b\u662f\u9690\u9a6c\u5c14\u79d1\u592b\u6a21\u578b (HMM, Hidden Markov Model). \u53e6\u4e00\u4e2a\u5e8f\u5217\u6a21\u578b\u7684\u4f8b\u5b50\n\u662f\u6761\u4ef6\u968f\u673a\u573a (CRF, Conditional Random Field).\n\n\u9012\u5f52\u795e\u7ecf\u7f51\u7edc\u662f\u6307\u53ef\u4ee5\u4fdd\u5b58\u67d0\u79cd\u72b6\u6001\u7684\u795e\u7ecf\u7f51\u7edc. \u6bd4\u5982\u8bf4, \u7f51\u7edc\u4e0a\u4e2a\u65f6\u523b\u7684\u8f93\u51fa\u53ef\u4ee5\u4f5c\u4e3a\u4e0b\u4e2a\n\u65f6\u523b\u7684\u8f93\u5165, \u8fd9\u6837\u4fe1\u606f\u5c31\u53ef\u4ee5\u901a\u8fc7\u5e8f\u5217\u5728\u7f51\u7edc\u4e2d\u4e00\u76f4\u5f80\u540e\u4f20\u9012. \u5bf9\u4e8eLSTM (Long-Short \nTerm Memory) \u6765\u8bf4, \u5e8f\u5217\u4e2d\u7684\u6bcf\u4e2a\u5143\u7d20\u90fd\u6709\u4e00\u4e2a\u76f8\u5e94\u7684\u9690\u72b6\u6001 $h_t$, \u8be5\u9690\u72b6\u6001\n\u539f\u5219\u4e0a\u53ef\u4ee5\u5305\u542b\u5e8f\u5217\u5f53\u524d\u7ed3\u70b9\u4e4b\u524d\u7684\u4efb\u4e00\u8282\u70b9\u7684\u4fe1\u606f. \u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u9690\u85cf\u72b6\u6001\u6765\u9884\u6d4b\u8bed\u8a00\u6a21\u578b\n\u4e2d\u7684\u5355\u8bcd, \u8bcd\u6027\u6807\u7b7e\u4ee5\u53ca\u5176\u4ed6\u5404\u79cd\u5404\u6837\u7684\u4e1c\u897f.\n\n\nPytorch \u4e2d\u7684 LSTM\n~~~~~~~~~~~~~~~~~~\n\n\u5f00\u59cb\u4f8b\u5b50\u4e4b\u524d,\u6709\u51e0\u4e2a\u70b9\u8bf4\u660e\u4e00\u4e0b. Pytorch \u4e2d, LSTM \u7684\u6240\u6709\u7684\u5f62\u5f0f\u56fa\u5b9a\u4e3a3D \u7684 tensor.\n\u6bcf\u4e2a\u7ef4\u5ea6\u6709\u56fa\u5b9a\u7684\u8bed\u4e49\u542b\u4e49, \u4e0d\u80fd\u4e71\u6389. \u5176\u4e2d\u7b2c\u4e00\u7ef4\u662f\u5e8f\u5217\u672c\u8eab, \u7b2c\u4e8c\u7ef4\u4ee5 mini-batch \u5f62\u5f0f\n\u6765\u7d22\u5f15\u5b9e\u4f8b, \u800c\u7b2c\u4e09\u7ef4\u5219\u7d22\u5f15\u8f93\u5165\u7684\u5143\u7d20. \u56e0\u4e3a\u6211\u4eec\u6ca1\u6709\u8ba8\u8bba\u8fc7 mini-batch, \u6240\u4ee5\u5728\u8fd9\u91cc\u6211\u4eec\n\u5047\u8bbe\u7b2c\u4e8c\u7ef4\u7684\u7ef4\u5ea6\u603b\u662f1. \u5982\u679c\u6211\u4eec\u60f3\u5728\u53e5\u5b50 \"The cow jumped\" \u4e0a\u8fd0\u884c\u4e00\u4e2a\u5e8f\u5217\u6a21\u578b, \u6a21\u578b\n\u7684\u8f93\u5165\u7c7b\u4f3c\u8fd9\u6837:\n\n\\begin{align}\\begin{bmatrix}\n   \\overbrace{q_\\text{The}}^\\text{row vector} \\\\\n   q_\\text{cow} \\\\\n   q_\\text{jumped}\n   \\end{bmatrix}\\end{align}\n\n\u9664\u4e86\u6709\u4e00\u4e2a\u989d\u5916\u7684\u5927\u5c0f\u4e3a1\u7684\u7b2c\u4e8c\u7ef4\u5ea6.\n\n\u6b64\u5916, \u4f60\u8fd8\u53ef\u4ee5\u5411\u7f51\u7edc\u9010\u4e2a\u8f93\u5165\u5e8f\u5217, \u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b, \u7b2c\u4e00\u4e2a\u8f74\u7684\u5927\u5c0f\u4e5f\u662f1.\n\n\u6765\u770b\u4e00\u4e2a\u7b80\u5355\u7684\u4f8b\u5b50.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# \u4f5c\u8005: Robert Guthrie\n\nimport torch\nimport torch.autograd as autograd\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\ntorch.manual_seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "lstm = nn.LSTM(3, 3)  # \u8f93\u5165\u7ef4\u5ea6\u662f3, \u8f93\u51fa\u7ef4\u5ea6\u4e5f\u662f3\ninputs = [autograd.Variable(torch.randn((1, 3)))\n          for _ in range(5)]  # \u6784\u9020\u4e00\u4e2a\u957f\u5ea6\u4e3a5\u7684\u5e8f\u5217\n\n# \u521d\u59cb\u5316\u9690\u85cf\u72b6\u6001\nhidden = (autograd.Variable(torch.randn(1, 1, 3)),\n          autograd.Variable(torch.randn((1, 1, 3))))\nfor i in inputs:\n    # \u5c06\u5e8f\u5217\u7684\u5143\u7d20\u9010\u4e2a\u8f93\u5165\u5230LSTM\n    # \u7ecf\u8fc7\u6bcf\u6b65\u64cd\u4f5c,hidden \u7684\u503c\u5305\u542b\u4e86\u9690\u85cf\u72b6\u6001\u7684\u4fe1\u606f\n    out, hidden = lstm(i.view(1, 1, -1), hidden)\n\n# \u53e6\u5916, \u6211\u4eec\u8fd8\u53ef\u4ee5\u4e00\u6b21\u5bf9\u6574\u4e2a\u5e8f\u5217\u8fdb\u884c\u8bad\u7ec3. LSTM \u8fd4\u56de\u7684\u7b2c\u4e00\u4e2a\u503c\u8868\u793a\u6240\u6709\u65f6\u523b\u7684\u9690\u72b6\u6001\u503c,\n# \u7b2c\u4e8c\u4e2a\u503c\u8868\u793a\u6700\u8fd1\u7684\u9690\u72b6\u6001\u503c (\u56e0\u6b64\u4e0b\u9762\u7684 \"out\"\u7684\u6700\u540e\u4e00\u4e2a\u503c\u548c \"hidden\" \u7684\u503c\u662f\u4e00\u6837\u7684).\n# \u4e4b\u6240\u4ee5\u8fd9\u6837\u8bbe\u8ba1, \u662f\u4e3a\u4e86\u901a\u8fc7 \"out\" \u7684\u503c\u6765\u83b7\u53d6\u6240\u6709\u7684\u9690\u72b6\u6001\u503c, \u800c\u7528 \"hidden\" \u7684\u503c\u6765\n# \u8fdb\u884c\u5e8f\u5217\u7684\u53cd\u5411\u4f20\u64ad\u8fd0\u7b97, \u5177\u4f53\u65b9\u5f0f\u5c31\u662f\u5c06\u5b83\u4f5c\u4e3a\u53c2\u6570\u4f20\u5165\u540e\u9762\u7684 LSTM \u7f51\u7edc.\n\n# \u589e\u52a0\u989d\u5916\u7684\u7b2c\u4e8c\u4e2a\u7ef4\u5ea6\ninputs = torch.cat(inputs).view(len(inputs), 1, -1)\nhidden = (autograd.Variable(torch.randn(1, 1, 3)), autograd.Variable(\n    torch.randn((1, 1, 3))))  # \u6e05\u7a7a\u8f93\u51fa\u9690\u72b6\u6001\nout, hidden = lstm(inputs, hidden)\nprint(out)\nprint(hidden)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u4f8b\u5b50: \u7528 LSTM \u6765\u8fdb\u884c\u8bcd\u6027\u6807\u6ce8\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\u5728\u8fd9\u90e8\u5206, \u6211\u4eec\u5c06\u4f1a\u4f7f\u7528\u4e00\u4e2a LSTM \u7f51\u7edc\u6765\u8fdb\u884c\u8bcd\u6027\u6807\u6ce8. \u5728\u8fd9\u91cc\u6211\u4eec\u4e0d\u4f1a\u7528\u5230\u7ef4\u7279\u6bd4\u7b97\u6cd5,\n\u524d\u5411\u540e\u5411\u7b97\u6cd5\u6216\u8005\u4efb\u4f55\u7c7b\u4f3c\u7684\u7b97\u6cd5, \u800c\u662f\u5c06\u8fd9\u90e8\u5206\u5185\u5bb9\u4f5c\u4e3a\u4e00\u4e2a (\u6709\u6311\u6218) \u7684\u7ec3\u4e60\u7559\u7ed9\u8bfb\u8005,\n\u5e0c\u671b\u8bfb\u8005\u5728\u4e86\u89e3\u4e86\u8fd9\u90e8\u5206\u7684\u5185\u5bb9\u540e\u80fd\u591f\u5b9e\u73b0\u5982\u4f55\u5c06\u7ef4\u7279\u6bd4\u7b97\u6cd5\u5e94\u7528\u5230 LSTM \u7f51\u7edc\u4e2d\u6765.\n\n\u6574\u4e2a\u6a21\u578b\u7684\u53c2\u6570\u5b9a\u4e49\u5982\u4e0b: \u8f93\u5165\u7684\u53e5\u5b50\u5b9a\u4e49\u4e3a $w_1, \\dots, w_M$, \u5176\u4e2d\u52a8\u8bcd\u5b9a\u4e49\n\u4e3a $w_1, \\dots, w_M$, \u6807\u7b7e\u96c6\u5408\u5b9a\u4e49\u4e3a $T$, \u5355\u8bcd $w_i$ \u7684\u5b9e\u9645\n\u6807\u7b7e\u4e3a $y_i$. \u5b9a\u4e49\u5355\u8bcd $w_i$ \u7684\u9884\u6d4b\u6807\u7b7e\u4e3a $\\hat{y}_i$.\n\n\u8fd9\u662f\u4e00\u4e2a\u7ed3\u6784\u9884\u6d4b\u6a21\u578b, \u6211\u4eec\u7684\u8f93\u51fa\u662f\u4e00\u4e2a\u5e8f\u5217 $\\hat{y}_1, \\dots, \\hat{y}_M$,\n\u5176\u4e2d $\\hat{y}_i \\in T$.\n\n\u5728\u8fdb\u884c\u9884\u6d4b\u65f6, \u9700\u5c06\u53e5\u5b50\u6bcf\u4e2a\u8bcd\u8f93\u5165\u5230\u4e00\u4e2a LSTM \u7f51\u7edc\u4e2d. \u5c06\u65f6\u523b $i$ \u7684\u9690\u72b6\u6001\u6807\u8bb0\n\u4e3a $h_i$. \u540c\u6837\u5730, \u5bf9\u6bcf\u4e2a\u6807\u7b7e\u8d4b\u4e00\u4e2a\u72ec\u4e00\u65e0\u4e8c\u7684\u7d22\u5f15 (\u7c7b\u4f3c word embeddings \u90e8\u5206\nword\\_to\\_ix \u7684\u8bbe\u7f6e). \u7136\u540e\u5c31\u5f97\u5230\u4e86 $\\hat{y}_i$ \u7684\u9884\u6d4b\u89c4\u5219:\n\n\\begin{align}\\hat{y}_i = \\text{argmax}_j \\  (\\log \\text{Softmax}(Ah_i + b))_j\\end{align}\n\n\u5373\u5148\u5bf9\u9690\u72b6\u6001\u8fdb\u884c\u4e00\u4e2a\u4eff\u5c04\u53d8\u6362, \u7136\u540e\u8ba1\u7b97\u4e00\u4e2a\u5bf9\u6570 softmax, \u6700\u540e\u5f97\u5230\u7684\u9884\u6d4b\u6807\u7b7e\u5373\u4e3a\u5bf9\u6570\nsoftmax \u4e2d\u6700\u5927\u7684\u503c\u5bf9\u5e94\u7684\u6807\u7b7e. \u6ce8\u610f, \u8fd9\u4e5f\u610f\u5473\u7740 $A$ \u7a7a\u95f4\u7684\u7ef4\u5ea6\u662f $|T|$.\n\n\n\u51c6\u5907\u6570\u636e:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def prepare_sequence(seq, to_ix):\n    idxs = [to_ix[w] for w in seq]\n    tensor = torch.LongTensor(idxs)\n    return autograd.Variable(tensor)\n\n\ntraining_data = [\n    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n]\nword_to_ix = {}\nfor sent, tags in training_data:\n    for word in sent:\n        if word not in word_to_ix:\n            word_to_ix[word] = len(word_to_ix)\nprint(word_to_ix)\ntag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}\n\n# \u5b9e\u9645\u4e2d\u901a\u5e38\u4f7f\u7528\u66f4\u5927\u7684\u7ef4\u5ea6\u598232\u7ef4, 64\u7ef4.\n# \u8fd9\u91cc\u6211\u4eec\u4f7f\u7528\u5c0f\u7684\u7ef4\u5ea6, \u4e3a\u4e86\u65b9\u4fbf\u67e5\u770b\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6743\u91cd\u7684\u53d8\u5316.\nEMBEDDING_DIM = 6\nHIDDEN_DIM = 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u6784\u9020\u6a21\u578b:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class LSTMTagger(nn.Module):\n\n    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n        super(LSTMTagger, self).__init__()\n        self.hidden_dim = hidden_dim\n\n        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n\n        #  LSTM \u4ee5 word_embeddings \u4f5c\u4e3a\u8f93\u5165, \u8f93\u51fa\u7ef4\u5ea6\u4e3a hidden_dim \u7684\u9690\u72b6\u6001\u503c\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n\n        # \u7ebf\u6027\u5c42\u5c06\u9690\u72b6\u6001\u7a7a\u95f4\u6620\u5c04\u5230\u6807\u6ce8\u7a7a\u95f4\n        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n        self.hidden = self.init_hidden()\n\n    def init_hidden(self):\n        # \u5f00\u59cb\u65f6\u523b, \u6ca1\u6709\u9690\u72b6\u6001\n        # \u5173\u4e8e\u7ef4\u5ea6\u8bbe\u7f6e\u7684\u8be6\u60c5,\u8bf7\u53c2\u8003 Pytorch \u6587\u6863\n        # \u5404\u4e2a\u7ef4\u5ea6\u7684\u542b\u4e49\u662f (num_layers, minibatch_size, hidden_dim)\n        return (autograd.Variable(torch.zeros(1, 1, self.hidden_dim)),\n                autograd.Variable(torch.zeros(1, 1, self.hidden_dim)))\n\n    def forward(self, sentence):\n        embeds = self.word_embeddings(sentence)\n        lstm_out, self.hidden = self.lstm(\n            embeds.view(len(sentence), 1, -1), self.hidden)\n        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n        tag_scores = F.log_softmax(tag_space, dim=1)\n        return tag_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u8bad\u7ec3\u6a21\u578b:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\nloss_function = nn.NLLLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.1)\n\n# \u67e5\u770b\u4e0b\u8bad\u7ec3\u524d\u5f97\u5206\u7684\u503c\n# \u6ce8\u610f: \u8f93\u51fa\u7684 i,j \u5143\u7d20\u7684\u503c\u8868\u793a\u5355\u8bcd i \u7684 j \u6807\u7b7e\u7684\u5f97\u5206\ninputs = prepare_sequence(training_data[0][0], word_to_ix)\ntag_scores = model(inputs)\nprint(tag_scores)\n\nfor epoch in range(300):  # \u518d\u6b21\u8bf4\u660e\u4e0b, \u5b9e\u9645\u60c5\u51b5\u4e0b\u4f60\u4e0d\u4f1a\u8bad\u7ec3300\u4e2a\u5468\u671f, \u6b64\u4f8b\u4e2d\u6211\u4eec\u53ea\u662f\u6784\u9020\u4e86\u4e00\u4e9b\u5047\u6570\u636e\n    for sentence, tags in training_data:\n        # Step 1. \u8bf7\u8bb0\u4f4f Pytorch \u4f1a\u7d2f\u52a0\u68af\u5ea6\n        # \u6bcf\u6b21\u8bad\u7ec3\u524d\u9700\u8981\u6e05\u7a7a\u68af\u5ea6\u503c\n        model.zero_grad()\n\n        # \u6b64\u5916\u8fd8\u9700\u8981\u6e05\u7a7a LSTM \u7684\u9690\u72b6\u6001\n        # \u5c06\u5176\u4ece\u4e0a\u4e2a\u5b9e\u4f8b\u7684\u5386\u53f2\u4e2d\u5206\u79bb\u51fa\u6765\n        model.hidden = model.init_hidden()\n\n        # Step 2. \u51c6\u5907\u7f51\u7edc\u8f93\u5165, \u5c06\u5176\u53d8\u4e3a\u8bcd\u7d22\u5f15\u7684 Variables \u7c7b\u578b\u6570\u636e\n        sentence_in = prepare_sequence(sentence, word_to_ix)\n        targets = prepare_sequence(tags, tag_to_ix)\n\n        # Step 3. \u524d\u5411\u4f20\u64ad\n        tag_scores = model(sentence_in)\n\n        # Step 4. \u8ba1\u7b97\u635f\u5931\u548c\u68af\u5ea6\u503c, \u901a\u8fc7\u8c03\u7528 optimizer.step() \u6765\u66f4\u65b0\u68af\u5ea6\n        loss = loss_function(tag_scores, targets)\n        loss.backward()\n        optimizer.step()\n\n# \u67e5\u770b\u8bad\u7ec3\u540e\u5f97\u5206\u7684\u503c\ninputs = prepare_sequence(training_data[0][0], word_to_ix)\ntag_scores = model(inputs)\n# \u53e5\u5b50\u662f \"the dog ate the apple\", i,j \u8868\u793a\u5bf9\u4e8e\u5355\u8bcd i, \u6807\u7b7e j \u7684\u5f97\u5206.\n# \u6211\u4eec\u91c7\u7528\u5f97\u5206\u6700\u9ad8\u7684\u6807\u7b7e\u4f5c\u4e3a\u9884\u6d4b\u7684\u6807\u7b7e. \u4ece\u4e0b\u9762\u7684\u8f93\u51fa\u6211\u4eec\u53ef\u4ee5\u770b\u5230, \u9884\u6d4b\u5f97\n# \u5230\u7684\u7ed3\u679c\u662f0 1 2 0 1. \u56e0\u4e3a \u7d22\u5f15\u662f\u4ece0\u5f00\u59cb\u7684, \u56e0\u6b64\u7b2c\u4e00\u4e2a\u503c0\u8868\u793a\u7b2c\u4e00\u884c\u7684\n# \u6700\u5927\u503c, \u7b2c\u4e8c\u4e2a\u503c1\u8868\u793a\u7b2c\u4e8c\u884c\u7684\u6700\u5927\u503c, \u4ee5\u6b64\u7c7b\u63a8. \u6240\u4ee5\u6700\u540e\u7684\u7ed3\u679c\u662f DET\n# NOUN VERB DET NOUN, \u6574\u4e2a\u5e8f\u5217\u90fd\u662f\u6b63\u786e\u7684!\nprint(tag_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u7ec3\u4e60: \u4f7f\u7528\u5b57\u7b26\u7ea7\u7279\u5f81\u6765\u589e\u5f3a LSTM \u8bcd\u6027\u6807\u6ce8\u5668\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\u5728\u4e0a\u9762\u7684\u4f8b\u5b50\u4e2d, \u6bcf\u4e2a\u8bcd\u90fd\u6709\u4e00\u4e2a\u8bcd\u5d4c\u5165, \u4f5c\u4e3a\u5e8f\u5217\u6a21\u578b\u7684\u8f93\u5165. \u63a5\u4e0b\u6765\u8ba9\u6211\u4eec\u4f7f\u7528\u6bcf\u4e2a\u7684\u5355\u8bcd\u7684\n\u5b57\u7b26\u7ea7\u522b\u7684\u8868\u8fbe\u6765\u589e\u5f3a\u8bcd\u5d4c\u5165. \u6211\u4eec\u671f\u671b\u8fd9\u4e2a\u64cd\u4f5c\u5bf9\u7ed3\u679c\u80fd\u6709\u663e\u8457\u63d0\u5347, \u56e0\u4e3a\u50cf\u8bcd\u7f00\u8fd9\u6837\u7684\u5b57\u7b26\u7ea7\n\u4fe1\u606f\u5bf9\u4e8e\u8bcd\u6027\u6709\u5f88\u5927\u7684\u5f71\u54cd. \u6bd4\u5982\u8bf4, \u50cf\u5305\u542b\u8bcd\u7f00 *-ly* \u7684\u5355\u8bcd\u57fa\u672c\u4e0a\u90fd\u662f\u88ab\u6807\u6ce8\u4e3a\u526f\u8bcd.\n\n\u5177\u4f53\u64cd\u4f5c\u5982\u4e0b. \u7528 $c_w$ \u6765\u8868\u793a\u5355\u8bcd $w$ \u7684\u5b57\u7b26\u7ea7\u8868\u8fbe, \u540c\u4e4b\u524d\u4e00\u6837, \u6211\u4eec\u4f7f\n\u7528 $x_w$ \u6765\u8868\u793a\u8bcd\u5d4c\u5165. \u5e8f\u5217\u6a21\u578b\u7684\u8f93\u5165\u5c31\u53d8\u6210\u4e86 $x_w$ \u548c $c_w$ \n\u7684\u62fc\u63a5. \u56e0\u6b64, \u5982\u679c $x_w$ \u7684\u7ef4\u5ea6\u662f5, $c_w$ \u7684\u7ef4\u5ea6\u662f3, \u90a3\u4e48\u6211\u4eec\u7684 LSTM\n\u7f51\u7edc\u7684\u8f93\u5165\u7ef4\u5ea6\u5927\u5c0f\u5c31\u662f8.\n\n\u4e3a\u4e86\u5f97\u5230\u5b57\u7b26\u7ea7\u522b\u7684\u8868\u8fbe, \u5c06\u5355\u8bcd\u7684\u6bcf\u4e2a\u5b57\u7b26\u8f93\u5165\u4e00\u4e2a LSTM \u7f51\u7edc, \u800c $c_w$ \u5219\u4e3a\u8fd9\u4e2a\nLSTM \u7f51\u7edc\u6700\u540e\u7684\u9690\u72b6\u6001. \u4e00\u4e9b\u63d0\u793a:\n\n* \u65b0\u6a21\u578b\u4e2d\u9700\u8981\u4e24\u4e2a LSTM, \u4e00\u4e2a\u8ddf\u4e4b\u524d\u4e00\u6837, \u7528\u6765\u8f93\u51fa\u8bcd\u6027\u6807\u6ce8\u7684\u5f97\u5206, \u53e6\u5916\u4e00\u4e2a\u65b0\u589e\u52a0\u7684\u7528\u6765\n  \u83b7\u53d6\u6bcf\u4e2a\u5355\u8bcd\u7684\u5b57\u7b26\u7ea7\u522b\u8868\u8fbe.\n* \u4e3a\u4e86\u5728\u5b57\u7b26\u7ea7\u522b\u4e0a\u8fd0\u884c\u5e8f\u5217\u6a21\u578b, \u4f60\u9700\u8981\u7528\u5d4c\u5165\u7684\u5b57\u7b26\u6765\u4f5c\u4e3a\u5b57\u7b26 LSTM \u7684\u8f93\u5165.\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}