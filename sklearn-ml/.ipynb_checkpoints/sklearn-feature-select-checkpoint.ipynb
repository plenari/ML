{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn 内feature_select 的一些函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import feature_selection \n",
    "from  sklearn.datasets import load_iris\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=load_iris(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 移除低方差特征\n",
    "feature_selection.VarianceThreshold ,默认情况下移除方差为0的特征，一般应用于0,1特征\n",
    "$$Var[X]=p(1-p)$$\n",
    "\n",
    "我们要移除那些超过80%的数据都为1或0的特征：：\n",
    "我们可以使用阈值 .8 * (1 - .8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 1]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 1, 1]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance=feature_selection.VarianceThreshold(threshold=0.8*(1-0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [1, 0],\n",
       "       [0, 0],\n",
       "       [1, 1],\n",
       "       [1, 0],\n",
       "       [1, 1]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variance.fit_transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 单变量特征选择"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* feature_selection.SelectKBest\n",
    "* feature_selection.SelectPercentile  #选择得分较高的百分比\n",
    "* feature_selection.GenericUnivariateSelect \n",
    "* 可以设置不同的策略来进行单变量特征选择。同时不同的选择策略也能够使用超参数寻优，从而让我们找到最佳的单变量特征选择策略。\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SelectKBest \n",
    "\n",
    "feature_selection.SelectKBest(score_func=<function f_classif at 0x000001D3B92FB840>, k=10)\n",
    "\n",
    "SelectKBest(chi2, k=2).fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#top K\n",
    "kbest=feature_selection.SelectKBest(feature_selection.chi2,k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 3)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kbest.fit_transform(x,y).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 查找百分数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "per=feature_selection.SelectPercentile(percentile=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 3)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per.fit_transform(x,y).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 统计分析的结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对每个特征使用通用的单变量统计测试：  \n",
    "* feature_selection.SelectFdr #伪发现率(false discovery rate\n",
    "* feature_selection.SelectFpr  # 假正率(false positive rate)\n",
    "* feature_selection.SelectFwe   #族系误差率 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature_selection.chi2\n",
    "\n",
    "Compute chi-squared stats between each non-negative feature and class.\n",
    "\n",
    "* This score can be used to select the n_features features with the highest values for the test chi-squared statistic from X, which must\n",
    "contain only non-negative features such as booleans or frequencies (e.g., term counts in document classification), relative to thec \n",
    "lasses.\n",
    "\n",
    "* Recall that the chi-square test measures dependence between stochastic variables, so using this function \"weeds out\" the features that are the most likely to be independent of class and therefore irrelevant for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这些对象将得分函数作为输入，返回单变量的得分和 p 值 （或者仅仅是 SelectKBest 和 SelectPercentile 的分数）:\n",
    "\n",
    "* 对于回归: f_regression , mutual_info_regression\n",
    "* 对于分类: chi2 , f_classif , mutual_info_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selection.chi2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 稀疏数据的特征选择\n",
    "    如果你使用sparse数据（比如：数据表示使用sparse matrics），如果不对它作dense转换，那么只有chi2 适合处理这样的数据\n",
    "    注意：如果在分类问题上使用回归的scoring函数，你将得到无用的结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "总结：\n",
    "\n",
    "* f_classif: 在label/feature之间的方差分析(Analysis of Variance:ANOVA) 的F值，用于分类.\n",
    "* chi2: 非负feature的卡方检验, 用于分类.\n",
    "* f_regression: 在label/feature之间的F值，用于回归.\n",
    "* SelectKBest: 得到k个最高分的feature.\n",
    "* SelectFpr: 基于RPR（false positive rate）检验。\n",
    "* SelectFdr: 使用Benjamini-Hochberg过程。选择p值（false discovery rate）。\n",
    "* SelectFwe:\n",
    "* GenericUnivariateSelect: 可配置化的单变量特征选择器."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 递归特征消除(Recursive feature elimination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature_selection.RFE\n",
    "\n",
    "feature_selection.RFECV #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.SelectFromModel\n",
    "\n",
    "feature_selection.SelectFromModel(estimator, threshold=None, prefit=False)\n",
    "\n",
    "* threshold ： 阈值，string, float, optional default None 可以使用：median 或者 mean 或者 1.25 * mean 这种格式。\n",
    "* 如果使用参数惩罚设置为L1，则使用的阈值为1e-5，否则默认使用用mean\n",
    "*  prefit ：布尔，默认为False，是否为训练完的模型，（注意不能是cv，GridSearchCV或者clone the estimator得到的），如果是False的话则先fit，再transform。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1 基于L1的特征选择(L1-based feature selection)\n",
    "使用L1正则化的线性模型会得到稀疏解，当目标是降低维度的时候，可以使用sklearn中的给予L1正则化的线性模型，比如LinearSVC，逻辑回归，或者Lasso。\n",
    "\n",
    "但是要注意的是：在 SVM 和逻辑回归中，参数 C 是用来控制稀疏性的：小的 C 会导致少的特征被选择。使用 Lasso，alpha 的值越大，越少的特征会被选择。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\app\\anaconda\\lib\\site-packages\\sklearn\\utils\\__init__.py:54: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(mask.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(150, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "lsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(X, y)\n",
    "model = SelectFromModel(lsvc, prefit=True)\n",
    "X_new = model.transform(X)\n",
    "X_new.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2 随机稀疏模型(Randomized sparse models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3 基于树的特征选择(Tree-based feature selection)\n",
    "Tree类的算法包括决策树，随机森林等会在训练后，得出不同特征的重要程度，我们也可以利用这一重要属性对特征进行选择。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.4特征选择融入pipeline(Feature selection as part of a pipeline)\n",
    "\n",
    "    clf = Pipeline([\n",
    "      ('feature_selection', SelectFromModel(LinearSVC(penalty=\"l1\"))),\n",
    "      ('classification', RandomForestClassifier())\n",
    "    ])\n",
    "    clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 函数方法"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "feature_selection.base\n",
    "afeature_selection.f_classif\n",
    "feature_selection.f_oneway\n",
    "feature_selection.f_regression\n",
    "feature_selection.from_model\n",
    "feature_selection.mutual_info_\n",
    "feature_selection.mutual_info_classif\n",
    "feature_selection.mutual_info_regression\n",
    "feature_selection.rfe\n",
    "feature_selection.univariate_selection\n",
    "feature_selection.variance_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "CountVectorizer?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
